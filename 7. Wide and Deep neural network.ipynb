{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Neural Network\n",
    "\n",
    "## more than one attribute\n",
    "\n",
    "### Graduate School admission problem\n",
    "\n",
    "GPA, experience\n",
    "\n",
    "$ \\text{x_data} = \\left[ {\\begin{array}{cc}\n",
    "   2.1 & 0.1 \\\\\n",
    "   4.2 & 0.8 \\\\\n",
    "   3.1 & 0.9 \\\\\n",
    "   3.3 & 0.2 \\\\\n",
    "  \\end{array} } \\right]\n",
    "  $   $ \\text{y_data} = \\left[ {\\begin{array}{c}\n",
    "   0.0 \\\\\n",
    "   1.0 \\\\\n",
    "   0.0 \\\\\n",
    "   1.0\\\\\n",
    "  \\end{array} } \\right]\n",
    "  $\n",
    "  \n",
    "  $$ wX = \\hat{y} $$\n",
    "  $$ \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   w_1 &\n",
    "   w_2 \\\\\n",
    "  \\end{array} } \\right]\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   x_{11} & x_{21} & x_{31} & x_{41} \\\\\n",
    "   x_{12} & x_{22} & x_{32} & x_{42} \\\\\n",
    "  \\end{array} } \\right]\n",
    "   = \n",
    "   \\left[ {\\begin{array}{cccc}\n",
    "   y_1 &\n",
    "   y_2 &\n",
    "   y_3 &\n",
    "   y_4\n",
    "  \\end{array} } \\right]\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear = torch.nn.Linear(2,1)\n",
    "# y_pred = linear(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $$\n",
    "  \\left[ {\\begin{array}{c}\n",
    "   x_1 \\\\\n",
    "   x_2 \\\\\n",
    "   \\cdots \\\\\n",
    "   x_n \\\\\n",
    "  \\end{array} } \\right] \\implies \\text{Linear} \\implies \\text{Sigmoid} \\implies \\hat{y}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $$\n",
    "  \\ X \\implies \\text{Linear} \\implies \\text{Sigmoid} \\implies \\text{Linear} \\implies \\text{Sigmoid} \\implies \\cdots \\implies \\text{Linear} \\implies \\text{Sigmoid} \\implies \\hat{y}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "# l1 = torch.nn.Linear(2,4)\n",
    "# l2 = torch.nn.Linear(4,3)\n",
    "# l3 = torch.nn.Linear(3,1)\n",
    "\n",
    "# out1 = sigmoid(l1(x_data))\n",
    "# out2 = sigmoid(l2(out1))\n",
    "# y_pred = sigmoid(l3(out2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function causes Vanishing Gradient Problem\n",
    "\n",
    "gradient of sigmoid activation function, $ a * ( 1-a)$, always < 1, therefore decrease as it backpropagates !! \n",
    "\n",
    "<img src=\"http://img1.daumcdn.net/thumb/R1920x0/?fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F26698E4F592AE818420518\" >\n",
    "\n",
    "### Activation Functions\n",
    "<img src=\"http://rasbt.github.io/mlxtend/user_guide/general_concepts/activation-functions_files/activation-functions.png\">\n",
    "\n",
    "[visualization of Activation Functions](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -0.2941  0.4874  0.1803 -0.2929  0.0000  0.0015 -0.5312 -0.0333\n",
       " -0.8824 -0.1457  0.0820 -0.4141  0.0000 -0.2072 -0.7669 -0.6667\n",
       " -0.0588  0.8392  0.0492  0.0000  0.0000 -0.3055 -0.4927 -0.6333\n",
       " -0.8824 -0.1055  0.0820 -0.5354 -0.7778 -0.1624 -0.9240  0.0000\n",
       "  0.0000  0.3769 -0.3443 -0.2929 -0.6028  0.2846  0.8873 -0.6000\n",
       " [torch.FloatTensor of size 5x8], Variable containing:\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       " [torch.FloatTensor of size 5x1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:,0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:,[-1]]))\n",
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)\n",
    "x_data[:5,:], y_data[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n",
      "0 0.75724\n",
      "20 0.68613\n",
      "40 0.66029\n",
      "60 0.65089\n",
      "80 0.64742\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    if epoch%20 == 0 : \n",
    "        print(epoch, np.round(loss.data[0],5))\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 7-1\n",
    "\n",
    "- Classifying Diabetes with deep nets ( more than 10 layers )\n",
    "- Find other classification problems/datasets & try with deep network\n",
    "- Try different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
