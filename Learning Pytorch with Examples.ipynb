{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch by Examples\n",
    "\n",
    "### Warm up : Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30392761.437672347\n",
      "50 15474.150004829238\n",
      "100 536.2353764807206\n",
      "150 29.970848005444132\n",
      "200 1.9973594262231484\n",
      "250 0.1450698435523542\n",
      "300 0.011037104448403109\n",
      "350 0.0008647194278803149\n",
      "400 6.921296939526198e-05\n",
      "450 5.6398457095459384e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "epoch = []\n",
    "losses = []\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    \n",
    "    epoch.append(t)\n",
    "    losses.append(loss)\n",
    "    if t%50 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "plt.plot(epoch[300:], losses[300:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch - Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28519142.0\n",
      "50 10144.17578125\n",
      "100 236.7060089111328\n",
      "150 10.500627517700195\n",
      "200 0.6235345005989075\n",
      "250 0.04297851026058197\n",
      "300 0.0034494332503527403\n",
      "350 0.00046752727939747274\n",
      "400 0.00012692774180322886\n",
      "450 5.319342017173767e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# N is batch size; D_in is input dimension\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t%50 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch - Tensors & autogradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39001156.0\n",
      "50 16046.673828125\n",
      "100 864.838623046875\n",
      "150 78.19257354736328\n",
      "200 8.473577499389648\n",
      "250 0.9944329857826233\n",
      "300 0.12170836329460144\n",
      "350 0.015389843843877316\n",
      "400 0.0022659802343696356\n",
      "450 0.000508145778439939\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to comput gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad= True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand..\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch - defining new autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21570962.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ad43fd58f875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-ad43fd58f875>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mgrad_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1.e-6\n",
    "for t in range(500):\n",
    "    relu = MyReLU.apply\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 50 == 0 :\n",
    "        print(t, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
