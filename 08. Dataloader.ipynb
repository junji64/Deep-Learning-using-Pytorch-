{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Manual data feed\n",
    "\n",
    "이전의 예에서는 모든 자료를 한번에 다 읽고, 가중치 갱신을 위한 훈련의 모든 반복 때 마다 모든 데이타를 사용합니다. 주어진 자료의 크기가 작은 경우에는 별 문제 없이 진행가능하나, 그 크기가 너무 커지면 문제가 발생할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:,0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:,[-1]]))\n",
    "\n",
    "...   # Process all data at once\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)    # Feed all data at once\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the neural network terminology:\n",
    "\n",
    "- **epoch** = 모든 훈련자료에 대하여 한 번의 정방향 패스와 한 번의 역방향 패스 \n",
    "- **batch size** = 한 번의 정방향/역방향 패스에서의 훈련자료의 수. 배치 크기가 클수록 더 많은 메모리 공간이 필요합니다.\n",
    "- number of **iterations** = 패스의 수, 각 패스는 [배치 크기] 수의 자료를 사용합니다. 명확하게 하면, 하나의 패스 = 하나의 순방향 패스 + 하나의 역방향 패스 (우리는 순방향 패스와 역방향 패스를 두 개의 다른 패스로 계산하지 않습니다).\n",
    "\n",
    "예 : 1000 개의 훈련자료가 있고, 배치 크기가 500 인 경우, 1 epoch를 완료하는 데 2 회의 반복이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Iteration Batch using DataLoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for i, data inenumerate(train_loader, 0):\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    # Run your training process\n",
    "        print(epoch, i, \"inputs\", inputs.data, \"labels\", labels.data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\"Diabetes dataset.\"\"\"\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "                    # (1) Download, read data, etc.\n",
    "                    \n",
    "    def __getitem__(self, index): \n",
    "          return          # (2) Return one item on the index\n",
    "    \n",
    "    def __len__(self):\n",
    "          return          # (3) return the data length\n",
    "          \n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Run your training process            \n",
    "        print(epoch, i, \"inputs\", inputs.data, \"labels\", labels.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    # initialize yout data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:,0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:,[-1]])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, \n",
    "                          shuffle=True, num_workers=2)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Varable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward pass: compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        if epoch%20 == 0 : \n",
    "            print(epoch, np.round(loss.data[0],5))\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loaders\n",
    "\n",
    "- MNIST and FashionMNIST\n",
    "- COCO (Captioning and Detection)\n",
    "- LSUN Classification\n",
    "- ImageFolder\n",
    "- Imagenet-12\n",
    "- CIFAR10 and CIFAR100\n",
    "- STL10\n",
    "- SVHN\n",
    "- PhotoTour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8-1:\n",
    "- Check out existing datasets (torch.vision)\n",
    "- Buil DataLoader for\n",
    " - Titanic dataset : https://www.kaggle.com/c/titanic/download/train.csv\n",
    "- Build a classifier using the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
