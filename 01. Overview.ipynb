{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview (Machine Learning & Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://blogs.nvidia.com/wp-content/uploads/2016/07/Deep_Learning_Icons_R5_PNG.jpg.png\" cat[tion=\"from NDIVIA Blog\">\n",
    "\n",
    "- 인공지능(Artificial Intelligence) : 기계로 구현한 인간 지능\n",
    "\n",
    "- 기계학습(Machine Learning) : 인공지능을 구현한 방법들\n",
    "\n",
    "- 딥러닝(Deep Learning) : 기계학습을 구현한 한 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning 등장 배경\n",
    "\n",
    "- Algorithm 개선 \n",
    "      - Deep NN 에서의 vanishing gradient 해결 : CNN, RNN etc.\n",
    "- GPU\n",
    "- Big Data\n",
    "\n",
    "\n",
    "### Deep Learning 의 활용분야\n",
    "\n",
    "- 영상인식 및 묘사\n",
    "\n",
    "- \n",
    "\n",
    "### Deep Learning 프레임워크\n",
    "\n",
    "- **Numpy** : linear algebra libraries\n",
    "\n",
    "- **TensorFlow** (from Google): automatic gradient calculation\n",
    "\n",
    "- **Pytorch** (from facebook): automatic gradient calculation\n",
    " - Imperative: Python first / intuitive / Easy to use\n",
    " - GPU acceleration\n",
    " - Linux/OSX\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using Numpy\n",
    "\n",
    "$$ \\text{output} = x*y + z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788  0.95008842 -0.15135721]\n",
      " [-0.10321885  0.4105985   0.14404357  1.45427351]]\n",
      "[[ 0.76103773  0.12167502  0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502]]\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "0:00:00.000997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "x = np.random.randn(N,D)\n",
    "y = np.random.randn(N,D)\n",
    "z = np.random.randn(N,D)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = np.sum(b)\n",
    "\n",
    "grad_c = 1.0\n",
    "grad_b = grad_c * np.ones((N,D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_y = grad_a * y\n",
    "grad_x = grad_a * x\n",
    "\n",
    "print(grad_x)\n",
    "print(grad_y)\n",
    "print(grad_z)\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    x = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.float32)\n",
    "    z = tf.placeholder(tf.float32)\n",
    "\n",
    "    a = x * y\n",
    "    b = a + z\n",
    "    c = tf.reduce_sum(b)\n",
    "    \n",
    "grad_x, grad_y, grad_z = tf.gradients(c,[x,y,z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N,D),\n",
    "        y: np.random.randn(N,D),\n",
    "        z: np.random.randn(N,D)           \n",
    "    }\n",
    "    out = sess.run([c,grad_x,grad_y,grad_z],feed_dict = values)\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out\n",
    "\n",
    "print(grad_x_val)\n",
    "print(grad_y_val)\n",
    "print(grad_z_val)\n",
    "print(datetime.now()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid gradient at index 0 - expected shape [] but got [1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8cbc20b4ccc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#c.backward(gradient=torch.cuda.FloatTensor([1.0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid gradient at index 0 - expected shape [] but got [1]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "#x = Variable(torch.randn(N,D).cuda(),requires_grad=True)\n",
    "#y = Variable(torch.randn(N,D).cuda(),requires_grad=True)\n",
    "#z = Variable(torch.randn(N,D).cuda(),requires_grad=True)\n",
    "x = Variable(torch.randn(N,D),requires_grad=True)\n",
    "y = Variable(torch.randn(N,D),requires_grad=True)\n",
    "z = Variable(torch.randn(N,D),requires_grad=True)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "#c.backward(gradient=torch.cuda.FloatTensor([1.0]))\n",
    "c.backward(gradient=torch.FloatTensor([1.0]))\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
